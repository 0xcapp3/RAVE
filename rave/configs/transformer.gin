from __gin__ import dynamic_registration

from rave import attention
from rave import blocks

DROPOUT = .1
NUM_HEADS = 8
HEAD_DIM = 64
CACHE_SEQ_LEN = 128
FF_DIM = 2048

attention.Transformer:
    transformer_layer = @attention.TransformerLayer
    feed_forward_layer = @attention.OriginalFeedForwardModule
    num_layers = 1
attention.TransformerLayer:
    dim = %LATENT_SIZE
    attention = @attention.MultiHeadAlibiAttention
    dropout = %DROPOUT
    n_head = %NUM_HEADS
    head_dim = %HEAD_DIM

attention.MultiHeadAlibiAttention:
    n_head = %NUM_HEADS
    causal = True
    cache_seq_len = %CACHE_SEQ_LEN

attention.OriginalFeedForwardModule:
    model_dim = %LATENT_SIZE
    hidden_dim = %FF_DIM

blocks.EncoderV2.recurrent_layer = @attention.Transformer
blocks.GeneratorV2.recurrent_layer = @attention.Transformer